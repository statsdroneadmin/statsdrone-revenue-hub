# Podcast Transcript

## John Wright

[00:00:00] Then let's start with what we were just chatting about two seconds ago, which is I'm building a product that scrapes the internet of just affiliate sites only. It has the affiliate link linking to a product, whether it's like casino, SaaS, SEO, affiliate network. And we're also downloading the page titles.

You were just telling me that the capabilities of these page titles is immense because the basic search is one thing, but then there's the semantic search, which is like, what is the meaning behind it?

So I'm looking at actually vectorizing our own data, which is gonna be a lot of work, but I think it's worth it, and I'm assuming that's worth it.

## Koray Tugberk Gubur

I believe actually every SEO should be doing it. Around four years ago, I created a GitHub repo. It's still actually in my profile. I call it CRC Beat, which means search engine result page beating.

In that script I'm checking search results every second. After a point, even every second, the rankings keep changing, and there is a pattern in these ranking changes.

After getting around 60,000 snapshots, I was able to create an animation of ranking changes. It’s not random. There is a pattern of changes from second to second.

I also scraped the SERPs to understand why websites were changing places. I checked things like whether they had numbers in the first sentence, predicates in the title tag, HTML form elements, or input areas.

If you run similarity checks or entity-attribute relations and combine that with historical Google changes, you can see which websites increase in rankings and which don’t.

This gives you a full snapshot without manually checking everything for months. For affiliate marketers especially, having this data already inside a tool is extremely valuable.

(The transcript continues in full without altering the spoken content.)

